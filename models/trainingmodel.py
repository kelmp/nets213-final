# -*- coding: utf-8 -*-
"""TrainingModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UKlniNbSLi4Wog8F7RYPZ873AdZZNG1Y
"""

!pip3 install spacy==2.2.4
!python -m spacy download en

import spacy
import plac
import logging
import argparse
import sys
import os
import json
import pickle
import csv
import random
from spacy.util import minibatch, compounding
from pathlib import Path
import pandas as pd

# Takes in a filename is that is an MTurk output CSV, formats into the correct
# Python list of tuples format for SpaCy training set
def read_json(filename: str):
    data = []
    with open(filename, "r") as file:
        csv_in = csv.reader(file, delimiter=",", quotechar="\"", quoting=csv.QUOTE_ALL)
        next(csv_in) # skip the header row
        broken_rows = set()
        for row in csv_in:
            text = row[0]
            text_split = list(filter(lambda x: x != "", text.split(" ")))
            for action in json.loads(row[1]):
                entities = []
                for key in action.keys():
                    for index in action[key]["indices"]:
                        start = index[0]
                        start_word = text_split[start]
                        end = index[len(index) - 1]
                        end_word = text_split[end]

                        before_start = 0
                        for i in range(start):
                            before_start += text_split[i].count(start_word)

                        before_end = 0
                        for i in range(end):
                            before_end += text_split[i].count(end_word)
                        
                        start_index = find_nth(text, start_word, before_start)
                        end_index = find_nth(text, end_word, before_end) + len(end_word)
                        entity = (start_index, end_index, key)
                        if entity not in entities:
                            entities.append(entity)
                data.append((text, { "entities": entities }))
    return data

def find_nth(string: str, substring: str, n: int):
    if n == 0:
        return string.find(substring)
    else:
        return string.find(substring, find_nth(string, substring, n - 1) + 1)

def training_model(formatted_data):  
  # create a new model
  nlp = spacy.blank('en')
  print("Created blank 'en' model")
  if 'ner' not in nlp.pipe_names:
    ner = nlp.create_pipe('ner')
    nlp.add_pipe(ner)
  else:
    ner = nlp.get_pipe('ner')

  possible_labels = ["creature", "target", "attack", "spell", "feature", "attackRoll", "damageRoll", "savingThrow"]

  for i in possible_labels:
      ner.add_label(i)
  # Inititalizing optimizer
  optimizer = nlp.begin_training()

  # Get names of other pipes to disable them during training to train # only NER and update the weights
  n_iter = 5
  other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
  with nlp.disable_pipes(*other_pipes):  # only train NER
    for itn in range(n_iter):
      #random.shuffle(formatted_data)
      losses = {}
      batches = minibatch(formatted_data, size=compounding(4., 32., 1.001))
      for batch in batches:
        texts, annotations = zip(*batch) 
        # Updating the weights
        nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)
      print('Losses', losses)
          
  return nlp

# Test the trained model
def testing(nlp_model, test_text):  
  doc = nlp_model(test_text)
  print("Entities in '%s'" % test_text)
  for ent in doc.ents:
    print(ent.label_, ent.text)

# Save model 
def save_model(output_dir, nlp):
  output_dir = Path(output_dir)
  if not output_dir.exists():
    output_dir.mkdir()
  nlp.meta['name'] = 'dnd_entity_model'  # rename model
  nlp.to_disk(output_dir)
  print("Saved model to", output_dir)

# Load test data
test_df = pd.read_csv("combat_text.csv")
test_df.dropna(inplace=True)
test_df = test_df["text"].apply(lambda x: str(x))
test_df = test_df.head(20)

# Training with mturk and gold
formatted_data = read_json("mturk_and_gold.csv")
nlp_mturk_gold = training_model(formatted_data)

# Testing with mturk and gold
for i, v in test_df.items():
  test_text = v
  testing(nlp_mturk_gold, test_text)

save_model("mturk_gold_model_ner", nlp_mturk_gold)

# Downloading mturk and gold model
!zip -r /content/mturk_gold_model_ner.zip /content/mturk_gold_model_ner
from google.colab import files
files.download("/content/mturk_gold_model_ner.zip")

# Training with friends and gold
formatted_data = read_json("friend_and_gold.csv")
nlp_friend_gold = training_model(formatted_data)

# Testing with friends and gold
for i, v in test_df.items():
  test_text = v
  testing(nlp_friend_gold, test_text)

save_model("friends_gold_model_ner", nlp_friend_gold)

# Downloading friends and gold model
!zip -r /content/friends_gold_model_ner.zip /content/friends_gold_model_ner
from google.colab import files
files.download("/content/friends_gold_model_ner.zip")